{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from MPRA_predict.utils import *\n",
    "from MPRA_predict.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_df = pd.read_csv('/home/shared/enformer_data/human/sequences.bed', sep='\\t', header=None)\n",
    "bed_df.columns = ['chr', 'start', 'end', 'split']\n",
    "bed_df.to_csv('data/enformer_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr18</td>\n",
       "      <td>928386</td>\n",
       "      <td>1059458</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr4</td>\n",
       "      <td>113630947</td>\n",
       "      <td>113762019</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr11</td>\n",
       "      <td>18427720</td>\n",
       "      <td>18558792</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr16</td>\n",
       "      <td>85805681</td>\n",
       "      <td>85936753</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr3</td>\n",
       "      <td>158386188</td>\n",
       "      <td>158517260</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38166</th>\n",
       "      <td>chr19</td>\n",
       "      <td>33204702</td>\n",
       "      <td>33335774</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38167</th>\n",
       "      <td>chr14</td>\n",
       "      <td>41861379</td>\n",
       "      <td>41992451</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38168</th>\n",
       "      <td>chr19</td>\n",
       "      <td>30681544</td>\n",
       "      <td>30812616</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38169</th>\n",
       "      <td>chr14</td>\n",
       "      <td>61473198</td>\n",
       "      <td>61604270</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38170</th>\n",
       "      <td>chr2</td>\n",
       "      <td>129664471</td>\n",
       "      <td>129795543</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38171 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         chr      start        end  split\n",
       "0      chr18     928386    1059458  train\n",
       "1       chr4  113630947  113762019  train\n",
       "2      chr11   18427720   18558792  train\n",
       "3      chr16   85805681   85936753  train\n",
       "4       chr3  158386188  158517260  train\n",
       "...      ...        ...        ...    ...\n",
       "38166  chr19   33204702   33335774   test\n",
       "38167  chr14   41861379   41992451   test\n",
       "38168  chr19   30681544   30812616   test\n",
       "38169  chr14   61473198   61604270   test\n",
       "38170   chr2  129664471  129795543   test\n",
       "\n",
       "[38171 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bed_df = pd.read_csv('data/enformer_data.csv')\n",
    "bed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>genome</th>\n",
       "      <th>identifier</th>\n",
       "      <th>file</th>\n",
       "      <th>clip</th>\n",
       "      <th>scale</th>\n",
       "      <th>sum_stat</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF413AHU</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:K562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF868NHV</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:K562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF565YDB</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:K562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>625</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF971AHO</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:K562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  genome   identifier  \\\n",
       "121    121       0  ENCFF413AHU   \n",
       "122    122       0  ENCFF868NHV   \n",
       "123    123       0  ENCFF565YDB   \n",
       "625    625       0  ENCFF971AHO   \n",
       "\n",
       "                                                  file  clip  scale sum_stat  \\\n",
       "121  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "122  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "123  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "625  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "\n",
       "    description  \n",
       "121  DNASE:K562  \n",
       "122  DNASE:K562  \n",
       "123  DNASE:K562  \n",
       "625  DNASE:K562  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd.read_csv('/home/shared/enformer_data/human/targets.txt', sep='\\t')\n",
    "targets_K562 = targets[targets['description'] == 'DNASE:K562']\n",
    "targets_K562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38171/38171 [00:45<00:00, 840.79it/s]\n",
      "100%|██████████| 38171/38171 [00:45<00:00, 846.70it/s]\n",
      "100%|██████████| 38171/38171 [00:44<00:00, 856.32it/s]\n",
      "100%|██████████| 38171/38171 [00:46<00:00, 829.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import pyBigWig\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "def init_worker(bw_path):\n",
    "    # 在子进程初始化时打开 bigWig 文件，全局变量 bw 将在子进程内可用\n",
    "    global bw\n",
    "    bw = pyBigWig.open(bw_path)\n",
    "\n",
    "def process_row(row):\n",
    "    chrom, start, end = row\n",
    "    # 1024个bin,每个bin128bp,一共128*1024=131072bp\n",
    "    start = start\n",
    "    end = end\n",
    "    mean_values = bw.stats(chrom, start, end, nBins=1024, type='mean')\n",
    "    mean_values = mean_values[64: -64]\n",
    "    return mean_values\n",
    "\n",
    "def process_track(track_row, bed_df, num_workers=4):\n",
    "    track_index = track_row['index']\n",
    "    identifier = track_row['identifier']\n",
    "    bigwig_file = f'../../data/Enformer_tracks/downloads/{identifier}.bigWig'\n",
    "\n",
    "    # 将 bed_df 转换成一个简单的列表，以便传入 pool.imap\n",
    "    rows = [(r.chr, r.start, r.end) for r in bed_df.itertuples()]\n",
    "\n",
    "    # 使用多进程\n",
    "    # initializer 用于在子进程启动时运行 init_worker，将 bw 对象在子进程内打开\n",
    "    with Pool(processes=num_workers, initializer=init_worker, initargs=(bigwig_file,)) as pool:\n",
    "        # 使用 imap 异步迭代，配合 tqdm 显示进度条\n",
    "        labels = list(tqdm(pool.imap(process_row, rows), total=len(rows)))\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    np.save(f'data/labels_track_index_{track_index}.npy', labels)\n",
    "\n",
    "# 假设你有一个 targets_K562 是 DataFrame，里面有至少四行数据\n",
    "# 以及 bed_df 是你要处理的区间表格\n",
    "for i in range(4):\n",
    "    track_row = targets_K562.iloc[i]\n",
    "    process_track(track_row, bed_df, num_workers=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title `get_dataset(organism, subset, num_threads=8)`\n",
    "import glob\n",
    "import json\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# @title `get_targets(organism)`\n",
    "def get_targets(organism):\n",
    "  # targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
    "  targets_txt = f'/home/shared/enformer_data/{organism}/targets.txt'\n",
    "  return pd.read_csv(targets_txt, sep='\\t')\n",
    "\n",
    "\n",
    "def organism_path(organism):\n",
    "  # return os.path.join('gs://basenji_barnyard/data', organism)\n",
    "  return os.path.join('/home/shared/enformer_data', organism)\n",
    "\n",
    "\n",
    "def get_dataset(organism, subset, num_threads=8):\n",
    "  metadata = get_metadata(organism)\n",
    "  dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                    compression_type='ZLIB',\n",
    "                                    num_parallel_reads=num_threads)\n",
    "  dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                        num_parallel_calls=num_threads)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def get_metadata(organism):\n",
    "  # Keys:\n",
    "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
    "  # pool_width, crop_bp, target_length\n",
    "  path = os.path.join(organism_path(organism), 'statistics.json')\n",
    "  with tf.io.gfile.GFile(path, 'r') as f:\n",
    "    return json.load(f)\n",
    "\n",
    "\n",
    "def tfrecord_files(organism, subset):\n",
    "  # Sort the values by int(*).\n",
    "  return sorted(tf.io.gfile.glob(os.path.join(\n",
    "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "  \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "  feature_map = {\n",
    "      'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "      'target': tf.io.FixedLenFeature([], tf.string),\n",
    "  }\n",
    "  example = tf.io.parse_example(serialized_example, feature_map)\n",
    "  sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "  sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "  sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "  target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "  target = tf.reshape(target,\n",
    "                      (metadata['target_length'], metadata['num_targets']))\n",
    "  target = tf.cast(target, tf.float32)\n",
    "\n",
    "  return {'sequence': sequence,\n",
    "          'target': target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dataset = get_dataset('human', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(human_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = []\n",
    "for item in l:\n",
    "    seqs.append(item['sequence'].numpy())\n",
    "seqs = np.stack(seqs, axis=0)\n",
    "print(seqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/enformer_seqs.npy', seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for item in tqdm(l):\n",
    "    labels.append(item['target'].numpy())\n",
    "labels = np.stack(labels, axis=0)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/enformer_targets.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqs = np.load('data/enformer_seqs.npy')\n",
    "# seqs = onehots2strs(seqs)\n",
    "# save_txt('data/enformer_seqs.txt', seqs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
