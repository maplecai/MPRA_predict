import os
import sys
import yaml
import argparse
import logging
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
import torch.multiprocessing as mp
from torch import nn
from torch import distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import torch.utils.data
import torchinfo


script_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(script_dir)
sys.path.append(root_dir)
from MPRA_predict import models, datasets, metrics, utils
from MPRA_predict.utils import *


class Trainer:
    def __init__(self, config):
        # setup seed and distributed
        self.config = config
        utils.set_seed(config['seed'])
        logging.config.dictConfig(config['logger'])
        self.logger = logging.getLogger()

        self.distributed = config['distributed']

        if self.distributed is False:
            self.local_rank = 0
            gpu_id = config['gpu_ids'][0]
            self.device = torch.device(f'cuda:{gpu_id}')
            torch.cuda.set_device(self.device)
            self.logger.info(
                f"Start non-distributedd training on rank {self.local_rank}, {self.device}.")
        else:
            dist.init_process_group(backend='nccl', init_method='env://')
            self.local_rank = dist.get_rank()
            self.device = torch.device(f'cuda:{self.local_rank}')
            torch.cuda.set_device(self.device)
            self.logger.info(
                f"Start DDP training on rank {self.local_rank}, {self.device}.")

        if self.local_rank == 0:
            self.log = self.logger.info
        else:
            self.log = self.logger.debug

        # setup dataloader
        self.cell_types = config['cell_types']

        self.train_dataset = utils.init_obj(
            datasets, 
            config['train_dataset'])
        self.valid_dataset = utils.init_obj(
            datasets, 
            config['valid_dataset'])
        
        if self.distributed is False:
            self.train_loader = DataLoader(
                dataset=self.train_dataset, 
                batch_size=config['batch_size'],
                shuffle=True,
                num_workers=1)
            self.valid_loader = DataLoader(
                dataset=self.valid_dataset, 
                batch_size=config['batch_size'],
                shuffle=False,
                num_workers=1)
            
        # setup model and metric
        self.model = utils.init_obj(models, config['model'])

        # if config.get('load_saved_model', False) == True:
        #     saved_model_path = config['saved_model_path']
        #     state_dict = torch.load(saved_model_path)
        #     self.model.load_state_dict(state_dict)
        #     self.log(f"load saved model from {saved_model_path}")


        # if config.get('freeze_layers', False) == True:
        #     self.log(f"freeze conv layers")
        #     for name, param in self.model.named_parameters():
        #         if name.startwith('conv_layers'):
        #             param.requires_grad = False
        #         elif name.startwith('linear_layers'):
        #             param.requires_grad = True
        #         else:
        #             param.requires_grad = True

        if self.distributed is False:
            self.model = self.model.to(self.device)
        else:
            self.model = DDP(
                self.model, 
                device_ids=[self.local_rank], 
                find_unused_parameters=False)

        self.loss_func = utils.init_obj(metrics, config['loss_func'])
        trainable_params = filter(lambda p: p.requires_grad, self.model.parameters())
        self.optimizer = utils.init_obj(torch.optim, config['optimizer'], trainable_params)
        
        self.metric_funcs = [
            utils.init_obj(metrics, m) for m in config.get('metric_funcs', [])]
        self.metric_names = [
            m['type'] for m in config.get('metric_funcs', [])]
        self.metric_df = pd.DataFrame(
            index=self.cell_types, 
            columns=self.metric_names)

        if 'lr_scheduler' in config:
            self.lr_scheduler = utils.init_obj(
                torch.optim.lr_scheduler, 
                config['lr_scheduler'], 
                self.optimizer)
        else:
            self.lr_scheduler = torch.optim.lr_scheduler.ConstantLR(
                self.optimizer, 
                factor=1.0)

        if 'early_stopper' in config:
            self.early_stopper = utils.init_obj(
                utils, 
                config['early_stopper'], 
                save_dir=os.path.join(config['save_dir']), 
                trace_func=self.log)
        else:
            self.early_stopper = utils.EarlyStopping(patience=np.inf)


    def train(self):
        config = self.config
        num_epochs = config['num_epochs']
        batch_size = config['batch_size']
        num_valid_epochs = config['num_valid_epochs']
        
        input, label = next(iter(self.train_loader))
        input = to_device(input, self.device)
        self.log(torchinfo.summary(
            self.model, 
            input_data=[input], 
            verbose=0, 
            depth=5))
            
        self.log(f'cell_types = {self.cell_types}')
        self.log(f'len(train_dataset) = {len(self.train_dataset)}')
        self.log(f'len(valid_dataset) = {len(self.valid_dataset)}')
        self.log(f'len(train_loader) = {len(self.train_loader)}')
        self.log(f'len(valid_loader) = {len(self.valid_loader)}')
        self.log(f'num_epochs = {num_epochs}')
        self.log(f'batch_size = {batch_size}')
        self.log(f'start training')

        for epoch in range(num_epochs):
            self.epoch = epoch
            if self.distributed:
                self.train_sampler.set_epoch(epoch)

            # valid one epoch before training
            if (epoch == 0):
                self.valid_epoch()

            self.log(f'train on epoch {epoch}')
            self.train_epoch()
            
            if ((epoch+1) % num_valid_epochs == 0):
                self.log(f'valid on epoch {epoch}')
                self.valid_epoch()

                if (self.early_stopper is not None):
                    valid_pearson = self.metric_df.loc[self.cell_types, 'Pearson'].mean()
                    self.log(f'epoch = {epoch}, valid_pearson = {valid_pearson:.6f}, check for early stopping')
                    # use valid_pearson instread of valid_loss to check early stopping
                    self.early_stopper.check(valid_pearson)

                    if self.early_stopper.update_flag == True:
                        if self.local_rank == 0:
                            self.save_model()
                    if self.early_stopper.stop_flag == True:
                        break

        self.log(f'local_rank = {self.local_rank:1}, finish training.')

        if self.distributed:
            dist.destroy_process_group()


    def train_epoch(self, train_loader=None):
        if train_loader is None:
            train_loader = self.train_loader
        scheduler_interval = self.config.get('scheduler_interval', 'epoch')
        num_log_steps = self.config.get('num_log_steps', 0)
        train_steps = len(train_loader)
        train_loss = 0

        self.model.train()
        for batch_idx, (input, label) in enumerate(tqdm(train_loader, disable=(self.local_rank != 0))):
            input = to_device(input, self.device)
            label = to_device(label, self.device)
            out = self.model(input)
            loss = self.loss_func(out, label)
            self.optimizer.zero_grad()
            loss.backward()
            # nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)
            self.optimizer.step()
            
            if scheduler_interval == 'step':
                self.lr_scheduler.step(self.epoch + batch_idx/train_steps)
            
            train_loss += loss.item()
            if num_log_steps != 0 and batch_idx % num_log_steps == 0:
                self.logger.debug(
                    f'local_rank = {self.local_rank}, epoch = {self.epoch:3}, '
                    f'batch_idx = {batch_idx:3}, train_loss = {loss.item():.6f}')

        if scheduler_interval == 'epoch':
            self.lr_scheduler.step()

        train_loss = train_loss / train_steps
        self.log(f'local_rank = {self.local_rank:1}, epoch = {self.epoch:3}, train_loss = {train_loss:.6f}')


    def valid_epoch(self, valid_loader=None):
        if valid_loader is None:
            valid_loader = self.valid_loader
        torch.set_grad_enabled(False) # 代替with torch.no_grad()，避免多一层缩进，方便从train复制

        valid_steps = len(valid_loader)
        valid_loss = 0
        y_pred_list = []

        self.model.eval()
        for batch_idx, (input, label) in enumerate(tqdm(valid_loader, disable=(self.local_rank != 0))):
            input = to_device(input, self.device)
            label = to_device(label, self.device)
            out = self.model(input)
            loss = self.loss_func(out, label)
            valid_loss += loss
            y_pred_list.append(out.detach())

        valid_loss = valid_loss / valid_steps
        if self.distributed:
            dist.all_reduce(valid_loss, op=dist.ReduceOp.SUM)
            valid_loss = valid_loss / dist.get_world_size()
        self.log(f'local_rank = {self.local_rank:1}, epoch = {self.epoch:3}, valid_loss = {valid_loss:.6f}')

        y_pred_list = torch.cat(y_pred_list)
        if self.distributed:
            y_pred_list = self.dist_all_gather(y_pred_list).cpu()
        else:
            y_pred_list = y_pred_list.cpu()

        y_true_list = self.valid_dataset.labels
        assert y_pred_list.shape == y_true_list.shape

        for i, cell_type in enumerate(self.cell_types):
            log_message = f'cell_type = {cell_type:6}'
            if len(y_true_list.shape) == 1:
                indice = (self.valid_dataset.df['cell_type'] == cell_type)
                y_true_list_0 = y_true_list[indice]
                y_pred_list_0 = y_pred_list[indice]
            elif len(y_true_list.shape) == 2:
                y_true_list_0 = y_true_list[:, i]
                y_pred_list_0 = y_pred_list[:, i]
            else:
                raise ValueError(f'y_true_list.shape = {y_true_list.shape}')
            
            for metric_func in self.metric_funcs:
                metric_name = type(metric_func).__name__
                score = metric_func(y_pred_list_0, y_true_list_0)
                log_message += f', {metric_name} = {score:.6f}'
                self.metric_df.loc[cell_type, metric_name] = score
            self.log(log_message)
        torch.set_grad_enabled(True)


    def test(self, test_loader):
        torch.set_grad_enabled(False)
        # 代替with torch.no_grad()，避免多一层缩进，和train缩进一样，方便复制

        y_pred_list = []

        self.model.eval()
        for batch_idx, (input, label) in enumerate(tqdm(test_loader, disable=(self.local_rank != 0))):
            input = to_device(input, self.device)
            label = to_device(label, self.device)
            out = self.model(input)
            y_pred_list.append(out.detach())

        y_pred_list = torch.cat(y_pred_list).cpu().numpy()
        save_file_path = self.config['save_file_path']
        np.save(save_file_path, y_pred_list)
        torch.set_grad_enabled(True)


    def dist_all_gather(self, tensor):
        tensor_list = [torch.zeros_like(tensor, device=tensor.device) for _ in range(dist.get_world_size())]
        dist.all_gather(tensor_list, tensor)
        tensor_list = torch.cat(tensor_list)
        return tensor_list
    

    def save_model(self):
        checkpoint_dir = self.config['save_dir']

        # checkpoint = {
        #     'config': self.config,
        #     'epoch': self.epoch,
        #     'model_state_dict': self.model.state_dict(),
        #     'optimizer_state_dict': self.optimizer.state_dict(),
        #     }
        # checkpoint_path = os.path.join(checkpoint_dir, f'epoch{self.epoch}.pth')
        
        checkpoint = self.model.module.state_dict() if self.distributed else self.model.state_dict()
        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint.pth')
        torch.save(checkpoint, checkpoint_path)
        self.logger.debug(f'save model at {checkpoint_path}')


if __name__ == '__main__':
    args = argparse.ArgumentParser()
    args.add_argument('-c', '--config_path', type=str, default=None,
                      help='config file path',)
    args = args.parse_args()
    config_path = args.config_path

    config = utils.load_config(config_path)
    config = utils.process_config(config)

    trainer = Trainer(config)
    trainer.train()
